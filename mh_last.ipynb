{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumes = []\n",
    "with open('./resume.json') as f:\n",
    "    for line in f.readlines():\n",
    "            resumes.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vac = []\n",
    "with open('./vacancy.json') as f:\n",
    "    for line in f.readlines():\n",
    "            vac.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# препроцессинг hard skills\n",
    "текущая схема\n",
    "\n",
    "    1) разбить токенайзером\n",
    "    2) нормализовать слова\n",
    "    3) удалить стопслова\n",
    "    4) сделать сплит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "STOP_WORDS = stopwords.words('russian')\n",
    "SPLIT_SYMS = '[()!,.:;\\n]-/*'\n",
    "morph = pymorphy2.MorphAnalyzer()    \n",
    "NORMALIZE = lambda word: morph.parse(word)[0].normal_form # для ускорения работы\n",
    "def preprocess(line):\n",
    "    '''\n",
    "    input: str для препроцессинга (resume['best'])\n",
    "    output: list предпологаемых hard skills\n",
    "    '''\n",
    "    # токенайзер сплитанет по словам нормально (получше чем мы кривыми руками напишем)\n",
    "    tokenized = word_tokenize(line.lower())\n",
    "    \n",
    "    #приводим слова к начальной форме\n",
    "    tokenized = map(NORMALIZE, tokenized)\n",
    "    \n",
    "    # удаляем стопслова\n",
    "    tokenized = [x for x in tokenized if x not in STOP_WORDS]\n",
    "    \n",
    "    #делаем сплит\n",
    "    output = []                 # список хард скиллов\n",
    "    prev_index = -1\n",
    "    for index, i in enumerate(tokenized):\n",
    "        if len(i) == 1 and i in SPLIT_SYMS:\n",
    "            current_skill = tokenized[prev_index+1:index]\n",
    "            prev_index = index\n",
    "            if current_skill:   # не нужны пустые скиллы\n",
    "                output.append(current_skill)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128945/128945 [04:51<00:00, 442.01it/s]\n"
     ]
    }
   ],
   "source": [
    "hs = {}\n",
    "for resume in tqdm(resumes):\n",
    "    hs[resume['id']] = (preprocess(resume['best']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('skills.json', 'w')\n",
    "json.dump(hs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./skills.json') as f:\n",
    "    skills = (json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MY_STOPWORDS = (\n",
    "        ['ms', 'microsoft', 'etc'] + ['2008', '2000', '2003', '2007', '2010', '2012', '2011', '2009',\n",
    "       '2005', '2014', '2006', '2013']\n",
    "    +['1', '82', '', 'ad', '7', '2', '3', '8', 'cc', '77',\n",
    " '81', '4', '83', '5', '80', 'ос', '6', '3d', '2d', '10',\n",
    "'$', 'скс', 'сс', 'фсс', '98', '9']\n",
    "+['тч', 'др', 'тд', 'впр', 'пр', 'тп', 'тк']\n",
    "+['10$год', '5$год', '15$год', '2$год']\n",
    "+['предприятие$8', 'предприятие$82']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in skills:\n",
    "    for j in skills[i]:\n",
    "        curr_skill = []\n",
    "        for word in j:\n",
    "            word = ''.join([c for c in word if str.isalnum(c)])\n",
    "            if word not in MY_STOPWORDS:\n",
    "                curr_skill.append(word)\n",
    "        if curr_skill:\n",
    "            s.append('$'.join(curr_skill))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b = np.unique(s, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = list(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = sorted(c, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# кластеризация по levensthein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from levinshtein_distance import distance as levenshtein      \n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, dbscan\n",
    "SYNONYMS  = (\n",
    "    [('1c', '1с8'), ('1с', '1с8'), ('$1с', '1с8'), ('1с8', '1с82'), ('1с$82', '1с8'),\n",
    "     ('1с$8', '1с8'), ('1с$8', '1с$82'), ('1с$8', '1с$82'), ('1с', '1с82'), ('1с$8', '1с$8'),\n",
    "     ('1c', '1с82'), ('1с', '1с$8'), ('$1с', '1с$8'), ('$1с', '1с'), ('1c', '1с$82'),\n",
    "     ('$1с', '1с$82'), ('$1с', '1c'), ('$1с', '1с82'), ('1c', '1с$8'), ('1с$8', '1с82'), ('1с$82', '1с82'), ('1c', '1с'), ('1с', '1с$82'), ('c', 'с'), ('c$', 'с'), ('c', 'c$')]\n",
    "+['1с$бухгалтерия' '1сбухгалтерия' '1c$бухгалтерия']\n",
    "+[('exel', 'exсel'), ('excel', 'excell'), ('excell', 'exsel'), ('exel', 'exell'), ('exell', 'exsel'), ('exel', 'exsel'), ('excel', 'exсel'), ('excell', 'exсel'), ('exsel', 'exсel'), ('excell', 'exell'), ('excell', 'exel'), ('exell', 'exсel'), ('excel', 'exell'), ('excel', 'exel'), ('excel', 'exsel')]\n",
    "+[('$консультант$$', 'консультант'), ('$консультант$', 'консультант$'), ('$консультант$', '$консультант$$'), ('$консультант$$', 'консультант$'), ('$консультант$', 'консультант'), ('консультант', 'консультант$')]\n",
    "+[('1с$предприятие', '1спредприятие'), ('1c$предприятие', '1с$предприятие'), ('1c$предприятие', '1спредприятие')]\n",
    "+[('1с$77', '1с77'), ('1c$77', '1с$77'), ('1c$77', '1с77')]\n",
    "+[('html', 'html5')]\n",
    "+[('mac$os', 'macos')]\n",
    "+[('css', 'css3')]\n",
    "+[('mssql', 'mysql')]\n",
    "+[('vb', 'vba')]\n",
    "+[('3dmax', '3ds$max'), ('3d$max', '3ds$max'), ('3d$max', '3dmax')]\n",
    "+[('adobe$premier', 'adobe$premiere')]\n",
    "+[('sql', 'tsql')]\n",
    "+[('corel$draw', 'coreldraw')]\n",
    "+[('compas', 'kompas')]\n",
    "+[('office', 'offise')]\n",
    "+[('консультант$плюс', 'консультантплюс')]\n",
    "+[('водительский$удостоверение$категория$$', 'водительский$удостоверение$категория$b')]\n",
    "+[('mathlab', 'matlab')]\n",
    "+[('пк$$уверенный$пользователь', 'пк$уверенный$пользователь')]\n",
    "+[('fine$reader', 'finereader')]\n",
    "+[('power$point', 'powerpoint')]\n",
    "+[('компас$3d', 'компас3d')]\n",
    "+[('solid$works', 'solidworks')]\n",
    "+[('auto$cad', 'autocad')]\n",
    "+[('html', 'html5')]\n",
    "+[('open$office', 'openoffice')]\n",
    ")\n",
    "NOT_SYNONYMS = (\n",
    "[('1с8', 'c$'), ('c$', 'r'), ('1с82', 'pr'),\n",
    " ('1c', 'с'), ('pc', 'r'), ('c', 'pc'),\n",
    " ('pc', 'pr'), ('1с82', 'r'),\n",
    " ('1с', 'c'), ('1c', 'pc'), ('$1с', 'с'),\n",
    " ('c', 'pr'), ('1c', 'pr'), ('$1с', 'c'),\n",
    " ('1с$8', 'c'), ('1c', 'c'), ('1с82', 'c$'),\n",
    " ('c', 'r'), ('$1с', 'r'), ('pr', 'r'), ('1с$8', 'с'),\n",
    " ('1с8', 'pc'), ('1с82', 'pc'), ('$1с', 'pr'), ('1с8', 'r'), ('1с$82', 'c$'),\n",
    " ('1с', 'pr'), ('r', 'с'), ('1с', 'с'), ('1с$8', 'c$'), ('$1с', 'pc'), ('1с8', 'с'),\n",
    " ('1с$8', 'r'), ('1с', 'pc'), ('$1с', 'c$'), ('1с$82', 'r'), ('1с$8', 'pc'), ('1c', 'c$'),\n",
    " ('1с82', 'c'), ('1c', 'r'), ('1с$82', 'c'), ('1с8', 'c'), ('c$', 'pc'), ('1с', 'r'), ('pr', 'с'),\n",
    " ('1с$82', 'pc'), ('1с', 'c$'), ('c$', 'pr'), ('1с$8', 'pr'), ('pc', 'с'), ('1с$82', 'с'), ('1с8', 'pr'),\n",
    " ('1с$82', 'pr'), ('1с82', 'с')]\n",
    "+[('пк', 'тз'), ('пк', 'пфр'), ('пк', 'тз'), ('тз', 'тз'), ('пфр', 'тз')]\n",
    "+[('dns', 'dos'), ('dfd', 'dfs'), ('dos', 'ios'), ('dns', 'ios'), ('iis', 'ios'), ('dfs', 'ios'), ('dos', 'iis'), ('dns', 'iis'), ('dfd', 'dns'), ('dfs', 'dos'), ('dfd', 'iis'), ('dfs', 'dns'), ('dfs', 'iis'), ('dfd', 'ios'), ('dfd', 'dos')]\n",
    "+[('php', 'xp'), ('hp', 'php'), ('hp', 'xp')]\n",
    "+[('cms', 'css'), ('css3', 'wms'), ('css', 'wms'), ('cms', 'css3'), ('cms', 'wms')]\n",
    ")\n",
    "def lev_metric(x, y):\n",
    "    i, j = int(x[0]), int(y[0])     # extract indices\n",
    "    a, b = (data[i], data[j]) if data[i] < data[j] else (data[j], data[i])\n",
    "    if (a, b) in SYNONYMS:\n",
    "        return 0\n",
    "    if (a, b) in NOT_SYNONYMS:\n",
    "        return 100\n",
    "    return levenshtein(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([i[0] for i in c if i[1] > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(len(data)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = dbscan(X, metric=lev_metric, eps=1, min_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# переходим к построению графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in skills:\n",
    "    s = []\n",
    "    for j in skills[i]:\n",
    "        curr_skill = []\n",
    "        for word in j:\n",
    "            word = ''.join([c for c in word if str.isalnum(c)])\n",
    "            if word not in MY_STOPWORDS and word in data:\n",
    "                curr_skill.append(word)\n",
    "        if curr_skill:\n",
    "            s.append(' '.join(curr_skill))\n",
    "    skills[i] = ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(skills.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = CountVectorizer(token_pattern='',\n",
    "                    ngram_range=(1,1),\n",
    "                    tokenizer=lambda x:str.split(x, ' '),\n",
    "                    binary=True,\n",
    "                    analyzer='word',\n",
    "                    preprocessor=lambda x: x).fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a.dot(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {} # кандидат + список скилов\n",
    "for name, skills_list in skills.items():\n",
    "    curr_skill = []\n",
    "    for s in skills_list:\n",
    "        curr_skill.append(' '.join(s))\n",
    "    data[name] = curr_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do: добавить weights так, чтобы частотные слова учитывались с меньшим весом\n",
    "def metrics(d1, d2):\n",
    "    try:\n",
    "        return 1 - len(set(d1) & set(d2)) / len(set(d1) | set(d2))\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12497500/12497500 [03:43<00:00, 55924.52it/s]\n"
     ]
    }
   ],
   "source": [
    "pair_dist = [] # тройка id-id-расстояние\n",
    "for pair in tqdm(list(combinations(list(data.keys())[:5000], 2))):\n",
    "    m = metrics(data[pair[0]], data[pair[1]])\n",
    "    if m < 0.8:\n",
    "        pair_dist.append((pair[0], pair[1], m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import node2vec\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(pair_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities:   0%|          | 15/4352 [08:38<43:46:12, 36.33s/it]"
     ]
    }
   ],
   "source": [
    "mm = node2vec.Node2Vec(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
